{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cab5220-7c97-4a2e-8721-fe427bf0f309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning::Spark Session already created, some configs may not take.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred                    \n",
      "WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/software/spark-3.3.2-el8-x86_64/jars/spark-core_2.12-3.3.2.jar) to field java.util.regex.Pattern.pattern\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, cleaned_text: string, finished_unigrams: array<string>, finished_pos: array<string>, filtered_unigrams_by_pos: array<string>, joined_tokens: string, finished_final: array<string>, tf_features: vector, tf_idf_features: vector]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sparknlp\n",
    "from sparknlp.annotator import Tokenizer, PerceptronModel\n",
    "from sparknlp.base import DocumentAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import *\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import IDF\n",
    "from pyspark.ml.clustering import LDA\n",
    "from sparknlp.annotator import StopWordsCleaner\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql import functions as F\n",
    "from sparknlp.annotator import NGramGenerator\n",
    "from sparknlp.base import Finisher\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "\n",
    "spark = sparknlp.start()\n",
    "\n",
    "data = spark.read.csv(\"data_selfimprovement/cleaned_moral_scores.csv\", header= True).select([\"id\", \"cleaned_text\"])\n",
    "\n",
    "#Preprocessing\n",
    "documentAssembler = DocumentAssembler()\\\n",
    "     .setInputCol(\"cleaned_text\")\\\n",
    "     .setOutputCol('document')\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "            .setInputCols(['document'])\\\n",
    "            .setOutputCol('tokenized')\n",
    "\n",
    "normalizer = Normalizer() \\\n",
    "     .setInputCols(['tokenized']) \\\n",
    "     .setOutputCol('normalized') \n",
    "\n",
    "english = [\n",
    "    \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \n",
    "    \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"can\", \"cannot\", \"could\", \"did\", \n",
    "    \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \n",
    "    \"he\", \"her\", \"here\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"i\", \"if\", \"in\", \"into\", \"is\", \"it\", \n",
    "    \"its\", \"itself\", \"let\", \"me\", \"more\", \"most\", \"must\", \"my\", \"myself\", \"no\", \"nor\", \"not\", \"of\", \"off\", \"on\", \n",
    "    \"once\", \"only\", \"or\", \"other\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"some\", \"such\", \n",
    "    \"than\", \"that\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"these\", \"they\", \"this\", \"those\", \n",
    "    \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"were\", \"what\", \"when\", \"where\", \"which\", \n",
    "    \"while\", \"who\", \"whom\", \"why\", \"with\", \"would\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"will\", \"ll\", \n",
    "    \"re\", \"ve\", \"d\", \"s\", \"m\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \n",
    "    \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\", \"many\", \"us\", \"ok\", \"hows\", \"ive\", \"ill\", \"im\", \"cant\", \"topics\", \"topic\",\n",
    "    \"discuss\", \"thoughts\", \"yo\", \"thats\", \"whats\", \"lets\", \"nothing\", \"oh\", \"omg\", \n",
    "         \"things\", \"stuff\", \"yall\", \"haha\", \"yes\", \"no\", \"wo\", \"like\", 'good', \n",
    "         'work', 'got', 'going', 'dont', 'really', 'want', 'make', 'think', \n",
    "         'know', 'feel', 'people', 'life', \"getting\", \"lot\" \"great\", \"i\", \"me\", \n",
    "         \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \n",
    "        \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \n",
    "        \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \n",
    "        \"they\", \"them\", \"their\", \"theirs\",\"themselves\", \"what\", \"which\", \"who\", \n",
    "        \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \n",
    "        \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \n",
    "        \"does\", \"did\", \"doing\", \"will\", \"would\", \"should\", \"can\", \"could\", \"may\",\n",
    "        \"might\", \"must\", \"shall\", \"ought\", \"about\", \"above\", \"across\", \"after\", \n",
    "        \"against\", \"along\", \"amid\", \"among\", \"around\", \"as\", \"at\", \"before\", \"behind\",\n",
    "        \"below\", \"beneath\", \"beside\", \"between\", \"beyond\", \"but\", \"by\", \n",
    "        \"concerning\", \"considering\", \"despite\", \"down\", \"during\", \"except\", \"for\",\n",
    "        \"from\", \"in\", \"inside\", \"into\", \"like\", \"near\", \"next\", \"notwithstanding\",\n",
    "        \"of\", \"off\", \"on\", \"onto\", \"opposite\", \"out\", \"outside\", \"over\", \"past\",\n",
    "        \"regarding\", \"round\", \"since\", \"than\", \"through\", \"throughout\", \"till\", \n",
    "        \"to\", \"toward\", \"towards\", \"under\", \"underneath\", \"unlike\", \"until\", \"up\",\n",
    "        \"upon\", \"versus\", \"via\", \"with\", \"within\", \"without\", \"cant\", \"cannot\", \n",
    "        \"couldve\", \"couldnt\", \"didnt\", \"doesnt\", \"dont\", \"hadnt\", \"hasnt\", \n",
    "        \"havent\", \"hed\", \"hell\", \"hes\", \"howd\", \"howll\", \"hows\", \"id\", \"ill\", \n",
    "        \"im\", \"ive\", \"isnt\", \"itd\", \"itll\", \"its\", \"lets\", \"mightve\", \"mustve\", \n",
    "        \"mustnt\", \"shant\", \"shed\", \"shell\", \"shes\", \"shouldve\", \"shouldnt\", \n",
    "        \"thatll\", \"thats\", \"thered\", \"therell\", \"therere\", \"theres\", \"theyd\", \n",
    "        \"theyll\", \"theyre\", \"theyve\", \"wed\", \"well\", \"were\", \"weve\", \"werent\", \n",
    "        \"whatd\", \"whatll\", \"whatre\", \"whats\", \"whatve\", \"whend\", \"whenll\", \n",
    "        \"whens\", \"whered\", \"wherell\", \"wheres\", \"whichd\", \"whichll\", \"whichre\", \n",
    "        \"whichs\", \"whod\", \"wholl\", \"whore\", \"whos\", \"whove\", \"whyd\", \"whyll\", \n",
    "        \"whys\", \"wont\", \"wouldve\", \"wouldnt\", \"youd\", \"youll\", \"youre\", \"youve\",\n",
    "        \"f\", \"m\", \"because\", \"go\", \"lot\", \"get\", \"still\", \"way\", \"something\", \"much\",\n",
    "        \"thing\", \"someone\", \"person\", \"anything\", \"goes\", \"ok\", \"so\", \"just\", \"mostly\", \n",
    "        \"put\", \"also\", \"lots\", \"yet\", \"ha\", \"etc\", \"wasnt\", \"yeah\", \"okay\", \"lol\"]\n",
    "\n",
    "time = [\"monday\", \"tuesday\", \"wednesday\", \"thursday\", \"friday\", \"saturday\", \n",
    "        \"sunday\", \"morning\", \"noon\", \"afternoon\", \"evening\", \"night\", \"midnight\",\n",
    "        \"dawn\", \"dusk\", \"week\", \"weekend\", \"weekends\",\"weekly\", \"today\", \n",
    "        \"yesterday\", \"tomorrow\", \"yesterdays\", \"todays\", \"mondays\", \"tuesdays\",\n",
    "        \"wednesdays\", \"thursdays\", \"fridays\", \"saturdays\", \"sundays\", \"day\",\n",
    "        \"everyday\", \"daily\", \"workday\", 'time', 'month', 'year', 'pm', 'am', \"ago\",\n",
    "        \"year\"]\n",
    "\n",
    "reddit = [\"welcome\", \"hi\", \"hello\", \"sub\", \"reddit\", \"thanks\", \"thank\", \"maybe\",\n",
    "          \"wo30\", \"mods\", \"mod\", \"moderators\", \"subreddit\", \"btw\", \"aw\", \"aww\", \n",
    "          \"aww\", \"hey\", \"hello\", \"join\", \"joined\", \"post\", \"rselfimprovement\", \"op\"]\n",
    "\n",
    "topic_specific = [\"self\", \"improvement\", \"change\", \"action\",\n",
    "    'change', 'start', 'goal', 'habit', 'new', 'old', \n",
    "    'care', 'world', 'everyone', 'love', 'u', 'right', 'mean', 'matter',\n",
    "    'best', 'step', 'focus', 'hard', 'small',\n",
    "    'bad', 'help', 'time', 'problem', 'issue', 'advice',\n",
    "    'bit', 'experience', 'different',\n",
    "    'point', 'situation', 'negative', 'control', 'positive',\n",
    "    'use', 'question', 'idea', 'amp', 'medium', 'hour', 'day', 'minute',\n",
    "    'aaaaloot', \"selfimprovement\", \"_\", \"ampxb\"]\n",
    "\n",
    "stopwords = english + time + reddit + topic_specific\n",
    "\n",
    "stopwords_cleaner = StopWordsCleaner() \\\n",
    "     .setInputCols(['normalized']) \\\n",
    "     .setOutputCol('unigrams') \\\n",
    "     .setStopWords(stopwords)\n",
    "\n",
    "pos = PerceptronModel.load(\"/project/macs40123/spark-jars/pos_anc_en_3.0.0_3.0_1614962126490/\")\\\n",
    "      .setInputCols(\"document\", \"unigrams\")\\\n",
    "      .setOutputCol(\"pos\")\n",
    "\n",
    "finisher = Finisher().setInputCols(['unigrams', 'pos'])\n",
    "\n",
    "my_pipeline = Pipeline(\n",
    "      stages = [\n",
    "          documentAssembler,\n",
    "          tokenizer,\n",
    "          normalizer,\n",
    "          stopwords_cleaner,\n",
    "          #ngrammer,\n",
    "          pos,\n",
    "          finisher\n",
    "      ])\n",
    "\n",
    "pipelineModel = my_pipeline.fit(data)\n",
    "processed_data = pipelineModel.transform(data)\n",
    "processed_data.persist()\n",
    "\n",
    "#Filter by POS\n",
    "def filter_unigrams(finished_unigrams, finished_pos):\n",
    "    '''Filters individual words based on their POS tag'''\n",
    "    return [word for word, pos in zip(finished_unigrams, finished_pos)\n",
    "            if pos in ['JJ', 'NN', 'NNS', 'NNPS']]\n",
    "\n",
    "udf_filter_unigrams = F.udf(filter_unigrams, T.ArrayType(T.StringType()))\n",
    "\n",
    "processed_data = processed_data.withColumn('filtered_unigrams_by_pos', udf_filter_unigrams(\n",
    "                                                   F.col('finished_unigrams'),\n",
    "                                                   F.col('finished_pos')))\n",
    "\n",
    "#Now that POS was done, lemmatization makes more sense at this point\n",
    "\n",
    "#Merge tokens as just one string to be able to take it as a document in the new Pipeline\n",
    "tokens_as_string = F.udf(lambda x: ' '.join(x), T.StringType())\n",
    "processed_data = processed_data.withColumn('joined_tokens', tokens_as_string(F.col('filtered_unigrams_by_pos')))\n",
    "\n",
    "last_documentAssembler = DocumentAssembler() \\\n",
    "     .setInputCol('joined_tokens') \\\n",
    "     .setOutputCol('joined_document')\n",
    "\n",
    "last_tokenizer = Tokenizer() \\\n",
    "     .setInputCols(['joined_document']) \\\n",
    "     .setOutputCol('tokenized')\n",
    "     \n",
    "lemmatizer = LemmatizerModel.load(\"../models/lemma_ewt_en_3.4.3_3.0_1651416655397/\")\\\n",
    "      .setInputCols(\"tokenized\")\\\n",
    "      .setOutputCol(\"lemmatized\")\n",
    "\n",
    "stopwords_cleaner = StopWordsCleaner() \\\n",
    "     .setInputCols(['lemmatized']) \\\n",
    "     .setOutputCol('final') \\\n",
    "     .setStopWords(stopwords)\n",
    "\n",
    "last_finisher = Finisher() \\\n",
    "     .setInputCols(['final']) \\\n",
    "\n",
    "last_pipeline = Pipeline() \\\n",
    "     .setStages([last_documentAssembler,                  \n",
    "                 last_tokenizer,\n",
    "                 lemmatizer,\n",
    "                 stopwords_cleaner,\n",
    "                 last_finisher])\n",
    "\n",
    "final_data = last_pipeline.fit(processed_data).transform(processed_data)\n",
    "\n",
    "processed_data.unpersist()\n",
    "final_data.persist()\n",
    "\n",
    "## Vectorization\n",
    "#Apply TF-IDF filtering\n",
    "tfizer = CountVectorizer(inputCol='finished_final', outputCol='tf_features', minDF=0.01, maxDF=0.80, vocabSize= 2000)\n",
    "tf_model = tfizer.fit(final_data)\n",
    "tf_result = tf_model.transform(final_data)\n",
    "\n",
    "idfizer = IDF(inputCol='tf_features', outputCol='tf_idf_features')\n",
    "idf_model = idfizer.fit(tf_result)\n",
    "tfidf_result = idf_model.transform(tf_result)\n",
    "\n",
    "final_data.unpersist()\n",
    "tfidf_result.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b0e578-e01d-4d37-93a9-3791c25d73c8",
   "metadata": {},
   "source": [
    "Functions to evaluate and interpret model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ebfb836-5472-4074-8347-d0ff646c8a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tf_model.vocabulary\n",
    "\n",
    "def evaluate_model(model, data):\n",
    "    log_likelihood = model.logLikelihood(data)\n",
    "    perplexity = model.logPerplexity(data)\n",
    "    return log_likelihood, perplexity\n",
    "\n",
    "def get_words(token_list):\n",
    "     return [vocab[token_id] for token_id in token_list]\n",
    "udf_to_words = F.udf(get_words, T.ArrayType(T.StringType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b0856d",
   "metadata": {},
   "source": [
    "Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b9b7049-d851-4a4f-9d86-cf18ecd845ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/12/10 12:28:43 WARN OnlineLDAOptimizer: The input data is not directly cached, which may hurt performance if its parent RDDs are also uncached.\n",
      "24/12/10 12:28:43 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "24/12/10 12:28:43 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-173140938.8980072, 5.870863682478515)\n",
      "+-----+------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|topic|topicWords                                                                                                                          |\n",
      "+-----+------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0    |[therapy, emotion, relationship, feeling, therapist, child, emotional, family, depression, toxic, call, mom, god, response, sad]    |\n",
      "|1    |[book, game, video, read, value, youtube, purpose, opinion, play, waste, teach, boundaries, development, personal, character]       |\n",
      "|2    |[woman, man, social, media, sex, porn, anxiety, date, relationship, partner, addiction, drug, anxious, dude, male]                  |\n",
      "|3    |[skill, interest, activity, hobbies, practice, language, content, hobby, specific, music, internet, attention, online, movie, learn]|\n",
      "|4    |[fear, mind, moment, happiness, journey, success, mindset, failure, belief, present, future, true, reality, happy, power]           |\n",
      "|5    |[pain, choice, decision, shit, worth, trust, respect, fuck, mistakes, proud, nobody, treat, environment, suck, pressure]            |\n",
      "|6    |[weight, food, body, gym, exercise, sleep, bed, drink, task, list, routine, healthy, wake, water, energy]                           |\n",
      "|7    |[girl, guy, confidence, conversation, talk, nice, confident, friend, group, personality, comfortable, weird, fun, meet, comfort]    |\n",
      "|8    |[job, school, money, college, class, career, high, home, parent, business, car, degree, study, house, kid]                          |\n",
      "+-----+------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## LDA Model \n",
    "best_lda = LDA(k=9, maxIter=100, learningDecay=0.5, learningOffset = 50, featuresCol='tf_idf_features', topicConcentration= 0.04, seed=2503)\n",
    "best_lda_model = best_lda.fit(tfidf_result)\n",
    "print(evaluate_model(best_lda_model, tfidf_result))\n",
    "num_top_words = 15\n",
    "topics = best_lda_model.describeTopics(num_top_words).withColumn('topicWords', udf_to_words(F.col('termIndices')))\n",
    "topics.select('topic', 'topicWords').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb1bc92",
   "metadata": {},
   "source": [
    "See more words per topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cb2a9fd-a364-4f94-a01a-180205cf9d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|topic|topicWords                                                                                                                                                                                |\n",
      "+-----+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0    |[therapy, emotion, relationship, feeling, therapist, child, emotional, family, depression, toxic, call, mom, god, response, sad, angry, parent, voice, anger, mother]                     |\n",
      "|1    |[book, game, video, read, value, youtube, purpose, opinion, play, waste, teach, boundaries, development, personal, character, meaningful, knowledgeable, information, trouble, productive]|\n",
      "|2    |[woman, man, social, media, sex, porn, anxiety, date, relationship, partner, addiction, drug, anxious, dude, male, guy, instagram, account, sick, doctor]                                 |\n",
      "|3    |[skill, interest, activity, hobbies, practice, language, content, hobby, specific, music, internet, attention, online, movie, learn, project, community, sport, area, tv]                 |\n",
      "|4    |[fear, mind, moment, happiness, journey, success, mindset, failure, belief, present, future, true, reality, happy, power, process, feeling, challenge, desire, truth]                     |\n",
      "|5    |[pain, choice, decision, shit, worth, trust, respect, fuck, mistakes, proud, nobody, treat, environment, suck, pressure, tough, excuse, regret, wrong, move]                              |\n",
      "|6    |[weight, food, body, gym, exercise, sleep, bed, drink, task, list, routine, healthy, wake, water, energy, health, diet, eat, cold, smoke]                                                 |\n",
      "|7    |[girl, guy, confidence, conversation, talk, nice, confident, friend, group, personality, comfortable, weird, fun, meet, comfort, hair, kinda, attractive, face, club]                     |\n",
      "|8    |[job, school, money, college, class, career, high, home, parent, business, car, degree, study, house, kid, company, university, family, student, country]                                 |\n",
      "+-----+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_top_words = 20\n",
    "topics = best_lda_model.describeTopics(num_top_words).withColumn('topicWords', udf_to_words(F.col('termIndices')))\n",
    "topics.select('topic', 'topicWords').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9a585a9-150e-4d35-a513-365c1fb32cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|   id|        cleaned_text|   topicDistribution|\n",
      "+-----+--------------------+--------------------+\n",
      "|hk5r2|i had an appointm...|[0.25772546430017...|\n",
      "|iqimz|i created this si...|[0.00252735910519...|\n",
      "|pfzt5|hello everyone  i...|[0.00135854970223...|\n",
      "|pk714|i grew up with bo...|[0.15450430908514...|\n",
      "|q0q8x|i have to ask whe...|[0.16503666614454...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_lda = best_lda_model.transform(tfidf_result)\n",
    "df_lda.select('id','cleaned_text','topicDistribution').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0728a43d",
   "metadata": {},
   "source": [
    "Divide topic distribution into columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5525418-3068-436d-a1bb-c09d78df64bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|   id|              topic0|              topic1|              topic2|              topic3|              topic4|              topic5|              topic6|              topic7|              topic8|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|hk5r2| 0.25772546430017085|0.003775611054620...|0.003834471860638...|0.004778187658173109|0.006278209751250...|0.005236176118334888|  0.7082258193290417|0.005126673029224204|0.005019386898546132|\n",
      "|iqimz|0.002527359105196...|  0.2792810389826976|0.002009522777281...|  0.4931402500499202|   0.136811366893389|0.002745782924423326|0.002766163658410...| 0.07808753227730869|0.002630983331372194|\n",
      "|pfzt5|0.001358549702237...|  0.3197961334228271|0.001080143410732557|  0.2469973058890026| 0.33778426742075973|0.001475667046438...|0.001486846605781...|0.001444821088674...| 0.08857626541354667|\n",
      "|pk714| 0.15450430908514035|0.056011052485961235| 0.09434329691490141|4.114567686263439E-4|0.045412502665262484| 0.03804952394417015| 0.27420131799957015| 0.25448874948174166| 0.08257779065462617|\n",
      "|q0q8x|  0.1650366661445453| 0.04717491226650441|4.437053862972509E-4|  0.1408818151589804|7.277353457821108E-4|  0.1118039259895573|   0.251180172967735| 0.10969713144603371|  0.1730539352945646|\n",
      "|q412v| 0.10332231627652305|0.001419593950109...|0.001441714819561...|0.001796470979402...|  0.8842516658558592|0.001969038810882...| 0.00198421375904699|0.001927574644113...|0.001887410904500...|\n",
      "|q5mqk|0.001221093465594072|9.562431001043614E-4|9.711523000937587E-4|0.001210197437840...| 0.04932216110746519|  0.1557132915686081| 0.12645101386252114|  0.6628837138488896|0.001271133308883...|\n",
      "|q70xe|0.001799473906472...|0.001408726709736...|  0.1573649866922605|0.001783420730127816|0.002345442383241...|  0.8295413194203342|0.001969603282567...|0.001914079541421...|0.001872947333837...|\n",
      "|q7mrn|0.001957526948422923|0.001532573269778...|0.001556470834312...|  0.2804298560738402|0.002553195357567056| 0.09436094194429348|0.002143101440272...|  0.6134288252492526|0.002037508882259...|\n",
      "|qcsyp|0.001572689439459...|0.001231398011864...| 0.12313390254330056|0.001558348470736...| 0.00204763071195836| 0.18307074704504175| 0.34665293989470475|  0.1854778861996039| 0.15525445768333043|\n",
      "|qu825|  0.3554398101810883|0.001869026002894...|0.001898150688158...|0.002365217099370...|0.003117997238019952|0.002591902355809091| 0.30530802623130915|0.002538894221118653| 0.32487097598223136|\n",
      "|qxco0|0.002028731666081653|0.001588262815466434|0.001613039560312...|0.002010270514306728|0.002645563107475191|0.049715979200817856|  0.9361298973379499|0.002156693406901...|0.002111562390687484|\n",
      "|r89qc|0.001584591295794...|  0.2832813750944175| 0.10395592700726221|  0.2643863864372184|0.002064972651659149|0.001720966957872029|0.001734232807057...|0.001685617524582465| 0.33958593022413686|\n",
      "|ra0bn|0.002594148132665...|0.002031240832633226|0.002062895454668...| 0.23405233699179995|  0.7481443691839508|0.002816783453119...|0.002840041585022846|0.002758054831290...|0.002700129534849...|\n",
      "|rbi6h| 0.17261161037430292|0.001668261397077...|0.001694316287161...|0.002111243109610...|0.002776978557856331| 0.14546029805959368|  0.1427238351090382|  0.2084179151619202|  0.3225355419434394|\n",
      "|rd166|0.002246346681586...|0.001758884167236...|0.001786287494675...|0.002225969596655...|0.002926864862500325|  0.5550322292902866|0.002458447894401385|  0.2569306377545372| 0.17463433225812056|\n",
      "|rrhg8| 0.00935486997618677|0.007325250455485047|0.007439407556508802|0.009270027946356415|0.012204325997655028|   0.924481466590809|0.010239510812870571|0.009947010734195731|0.009738129929932737|\n",
      "|rvjcf|5.667266803722387E-4| 0.24549195909822258| 0.03260487909404526| 0.09964455193763029|7.396401210446662E-4|6.154587784494522E-4|  0.4195241280571632| 0.06906730533367936| 0.13174535089939282|\n",
      "|s0ruk|  0.1429546792289002|0.001532066626489...|0.001555944513193582|0.001938780123171...|  0.5187441024591785| 0.19367580751681723| 0.13548174951242045|0.002080337484662...|0.002036532535166...|\n",
      "|sa2de|  0.2092883314763254|0.002688595861583...|0.002730534914961...|0.003402553417406309|0.004481996211901434|0.003730991967885...|0.003758983257739...|  0.6502309272883814|  0.1196870856038151|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# UDF to convert topicDistribution vector into a list\n",
    "vector_to_array_udf = udf(lambda v: v.toArray().tolist(), ArrayType(DoubleType()))\n",
    "\n",
    "# Apply the UDF to get an array column of topic probabilities\n",
    "transformed_df = df_lda.withColumn(\"topic_probs\", vector_to_array_udf(col(\"topicDistribution\"))).select('id','topic_probs')\n",
    "\n",
    "# Create separate columns for each topic probability\n",
    "for topic_idx in range(9):\n",
    "    transformed_df = transformed_df.withColumn(f\"topic{topic_idx}\", col(\"topic_probs\")[topic_idx])\n",
    "\n",
    "# Drop the intermediate topic_probs column\n",
    "df_topics_dist = transformed_df.drop(\"topic_probs\")\n",
    "\n",
    "df_topics_dist.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "808ab98e-6f08-4b74-81cf-db29cea93b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 324:==================================================>     (9 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|     id|        cleaned_text|              topic0|              topic1|              topic2|              topic3|              topic4|              topic5|              topic6|              topic7|              topic8|\n",
      "+-------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|1001497|i am meeting a po...|  0.3824481514942221|8.811491667825105E-4| 0.26122686407739437|  0.1294414606072498|0.001468184607838...|0.001222078835549...|0.001232469331174...| 0.22090812056089248|0.001171521318897...|\n",
      "|1001uik|hey so basically ...|0.001057364556961...|8.279348934153635E-4|8.408480729673635E-4|0.001047938671797...| 0.16257124666766645| 0.14384746401178938|  0.5679406047467908| 0.12076582005689954|0.001100778321711601|\n",
      "|1002u8v|my life was prett...|0.003867201450561488|  0.3249579488235726| 0.15680105682502868|0.003833176045345751|0.005039147218404872|0.004200070393812821| 0.00423317055746198|0.004114236793612426| 0.49295399189219935|\n",
      "|1002wwt|welcome to this c...| 0.03146412066535376| 0.09219607822851121| 0.04152887358918939|  0.1738931352703811|  0.1156437438146735| 0.04688490196322914| 0.42825358339139447| 0.05105076028363407|0.019084802793633258|\n",
      "|10069yz|so theres this pr...|0.002728541950369...|0.002135829141655...|0.002169193789159187|  0.1695515029611205| 0.18167625847917745|0.002961974128198...|0.002984684955153...|  0.5280847437627748| 0.10770727083239172|\n",
      "+-------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Merge two dataframes \n",
    "df_merged = data.join(df_topics_dist, on=\"id\", how=\"inner\")\n",
    "df_merged.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ddbeac",
   "metadata": {},
   "source": [
    "Save to parquet files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9857776f-e5f6-4522-acda-7dd620dc154e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_merged.write.mode(\"overwrite\").parquet(\"data_selfimprovement/data_with_topics.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
